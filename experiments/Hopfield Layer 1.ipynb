{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5dbakk-Yxau"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade -q git+https://github.com/shuiruge/neural-ode.git@master\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from hopfield import (DiscreteTimeHopfieldLayer, DenseRecon, ModernDenseRecon,\n",
    "                      Conv2dRecon, RBMRecon, HebbianRBMRecon)\n",
    "\n",
    "from mnist import load_mnist\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global configurations\n",
    "\n",
    "IMAGE_SIZE = (32, 32)\n",
    "BINARIZE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0QNHlhGAPX2"
   },
   "outputs": [],
   "source": [
    "(x_train, _), _ = load_mnist(image_size=IMAGE_SIZE, binarize=BINARIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_type: str):\n",
    "    if model_type == 'dense':\n",
    "        model = tf.keras.Sequential([\n",
    "            DiscreteTimeHopfieldLayer(\n",
    "                DenseRecon(),\n",
    "                max_steps=100,\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'ca':\n",
    "        model = tf.keras.Sequential([\n",
    "            DiscreteTimeHopfieldLayer(\n",
    "                Conv2dRecon(\n",
    "                    filters=64,\n",
    "                    kernel_size=5,\n",
    "                    flatten=True,\n",
    "                ),\n",
    "                max_steps=100,\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'modern_dense':\n",
    "        model = tf.keras.Sequential([\n",
    "            DiscreteTimeHopfieldLayer(\n",
    "                ModernDenseRecon(\n",
    "                    memory=x_train[:100],\n",
    "                    #n=100,\n",
    "                    activation=tf.nn.softmax,\n",
    "                ),\n",
    "                max_steps=100,\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'rbm':\n",
    "        model = tf.keras.Sequential([\n",
    "            DiscreteTimeHopfieldLayer(\n",
    "                RBMRecon(latent_dim=256),\n",
    "                max_steps=100,\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    elif model_type == 'hrbm':\n",
    "        model = tf.keras.Sequential([\n",
    "            DiscreteTimeHopfieldLayer(\n",
    "                HebbianRBMRecon(latent_dim=256),\n",
    "                max_steps=100,\n",
    "                reg_factor=1),\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(f'Unknown model type: \"{model_type}\".')\n",
    "    model.compile(optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "kbqJL6p9GjLe",
    "outputId": "e92d7d0f-3a91-42fe-c48a-9b051c9b0ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7813/7813 [==============================] - 208s 27ms/step - loss: 7.0664e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f72c8c3ad90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model('rbm')\n",
    "X = x_train[:200].numpy()\n",
    "ds0 = tf.data.Dataset.from_tensor_slices(X)\n",
    "ds = ds0.shuffle(10000).repeat(5000).batch(128)\n",
    "model.fit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discrete_time_hopfield_layer (None, 1024)              1026025   \n",
      "=================================================================\n",
      "Total params: 1,026,025\n",
      "Trainable params: 1,026,024\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relax steps: 8\n",
      "2.0 => 0.0\n"
     ]
    }
   ],
   "source": [
    "## noised_X = X + np.random.normal(size=X.shape) * 0.2\n",
    "noised_X = np.where(np.random.random(size=X.shape) < 0.15, -X, X)\n",
    "recon_X = model.predict(noised_X)\n",
    "\n",
    "for layer in model.layers:\n",
    "    try:\n",
    "        print('Relax steps:', layer.final_step.numpy())\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "orig_err = noised_X - X\n",
    "err = recon_X - X\n",
    "print(f'{np.quantile(np.abs(orig_err), 0.99)} => '\n",
    "      f'{np.quantile(np.abs(err), 0.99)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Occupations\n",
    "\n",
    "#### Time\n",
    "\n",
    "1. Dense version is much faster than CNN version.\n",
    "\n",
    "#### Space\n",
    "\n",
    "1. CNN version needs only ~ 10^2 parameters. Recall that dense version needs 10^7 parameters.\n",
    "\n",
    "1. To reduce the number of variables in the dense version, use [prunning](https://stackoverflow.com/a/56451791/1218716) after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-noising\n",
    "\n",
    "1. However, CNN version is not robust to bit-flipping. Dense version is still very robust to it. Bit-flipping fails for CNN version hints that the information is not sparsely (distributedly) stored. Thus it cannot re-construct the original bit only from the information stored in its local neighbors. (Notice that bit-flipping creates non-smooth, thus always great, differences.) To see this, run the re-constructor on the bit-flipping noised inputs to see the 0.99-quantile of the re-construction error, comparing for both dense and CNN versions. Increasing filters will not change the failure.\n",
    "\n",
    "1. Dense version gains 99% re-construction even for 40% bit-flipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarization\n",
    "\n",
    "1. Binarization is also essential to CNN version. Non-binarized inputs won't de-noise. The essense of binarization maybe traced to the simplicity it leads to. Indeed, the final loss without binarization will be greater (0.03X -> 0.04X).\n",
    "\n",
    "1. Change X in {-1, 1} to {0, 1} causes error in de-noising. Don't know why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Time\n",
    "\n",
    "1. Discrete time version is much much faster in predicting. Without lossing the attributes the continuous version has\n",
    "\n",
    "1. Async update decreases the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete State\n",
    "\n",
    "1. Discrete time when using discrete time improves performance significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation between Continuous and Discrete Time\n",
    "\n",
    "The continous time version, i.e. ODE version, and the discrete time version, i.e. iterative equation version, are related, since both of them can be regarded as root-finding. That is, find the root $x_{\\star}$ s.t. $x - f(x) = 0$. This root is the fixed point, or relaxition phase. The ODE version uses the gradient descent method, and the iterative equation version uses definition. Both of them will find one of the many roots, which is ensured by the Lyapunov functions of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modern Version\n",
    "\n",
    "1. If the kernel is initialized by giving memory, the learning is redundant, and the performance is, as expected, splendid.\n",
    "\n",
    "1. However, if the kernel is initialized by Glorot initializer as usual, the learning is hard, and the performance is terrible.\n",
    "\n",
    "1. If the memory is a subset of the training data, then performance is terrible again.\n",
    "\n",
    "1. If the weight is pruned, then the memory is destroyed too. However, this will not happen to the \"traditional\" version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Cellular automata as convolutional neural networks (arXiv: 1809.02942).\n",
    "\n",
    "1. Blog: [Hopfield Networks is All You Need](https://ml-jku.github.io/hopfield-layers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EHL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
