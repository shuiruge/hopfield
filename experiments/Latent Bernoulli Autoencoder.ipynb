{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# For integer quantization:\n",
    "# !pip install tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from mnist import load_mnist, evaluate\n",
    "from hopfield.utils import softly_binarize, SoftBinarization\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global configurations\n",
    "\n",
    "IMAGE_SIZE = (16, 16)\n",
    "BINARIZE = True\n",
    "AUTOENCODER_TYPE = ('vanilla', 'variational')[1]\n",
    "NUM_DATA = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), _ = load_mnist(image_size=IMAGE_SIZE, binarize=BINARIZE)\n",
    "x_train = (x_train + 1) / 2  # x \\in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softly(fn):\n",
    "#     r\"\"\"Decorator that returns func(*x, **kwargs), with the gradients on the x\n",
    "#     :math:`\\partial f_i / \\partial x_j = \\delta_{i j}`, i.e. an unit Jacobian,\n",
    "#     or say, an identity vector-Jacobian-product.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def identity(*dy):\n",
    "#         if len(dy) == 1:\n",
    "#             dy = dy[0]\n",
    "#         return dy\n",
    "\n",
    "#     @tf.custom_gradient\n",
    "#     def softly_fn(*args, **kwargs):\n",
    "#         y = fn(*args, **kwargs)\n",
    "#         return y, identity\n",
    "\n",
    "#     return softly_fn\n",
    "\n",
    "\n",
    "# def softly_binarize(x, threshold, minval=0, maxval=1, from_logits=False):\n",
    "#     r\"\"\"Returns `maxval` if x > threshold else `minval`, element-wisely, with\n",
    "#     the gradients :math:`\\partial f_i / \\partial x_j = \\delta_{i j}`, i.e. an\n",
    "#     unit Jacobian.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x : tensor\n",
    "#     threshold : float\n",
    "#     minval : real number\n",
    "#     maxval : real number\n",
    "#     from_logits : bool, optional\n",
    "#         If true, then softly binarize sigmoid(x) instead of x.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     tensor\n",
    "#         The same shape and dtype as x.\n",
    "#     \"\"\"\n",
    "\n",
    "#     @softly\n",
    "#     def binarize(x):\n",
    "#         y = tf.where(x > threshold, maxval, minval)\n",
    "#         y = tf.cast(y, x.dtype)\n",
    "#         return y\n",
    "\n",
    "#     return binarize(tf.nn.sigmoid(x)) if from_logits else binarize(x)\n",
    "\n",
    "\n",
    "# class SoftBinarization(tf.keras.layers.Layer):\n",
    "#     \"\"\"For using in tf.keras.Sequential.\n",
    "\n",
    "#     If in training phase, then do nothing. Otherwise, make soft binarization.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     threshold : float\n",
    "#     from_logits : bool, optional\n",
    "#         If true, then softly binarize sigmoid(x) instead of x.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, threshold, from_logits=False, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.threshold = threshold\n",
    "#         self.from_logits = from_logits\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config()\n",
    "#         config['threshold'] = self.threshold\n",
    "#         config['from_logits'] = self.from_logits\n",
    "#         return config\n",
    "\n",
    "#     def call(self, x, training=None):\n",
    "#         if self.from_logits:\n",
    "#             x = tf.nn.sigmoid(x)\n",
    "#         if training:\n",
    "#             return x\n",
    "#         return softly_binarize(x, self.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    \"\"\"Multi-layer perceptron (MLP).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : [int]\n",
    "    activation : callable or string, optional\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['activation'] = self.activation\n",
    "        return config\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        layers = [tf.keras.layers.Dense(n, 'relu') for n in self.units[:-1]]\n",
    "        layers.append(tf.keras.layers.Dense(self.units[-1], self.activation))\n",
    "        self._ffn = tf.keras.Sequential(layers)\n",
    "        self._ffn.build(batch_input_shape)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self._ffn(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentBernoulliVanillaAutoencoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : [int]\n",
    "        Hidden units along the encoder. The decoder use symmetric structure.\n",
    "    activation : callable or string, optional\n",
    "        Final output activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "        self._encoder = MLP(units, 'sigmoid')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['activation'] = self.activation\n",
    "        return config\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        ambient_dim = batch_input_shape[-1]\n",
    "        units = self.units[::-1][1:] + [ambient_dim]  # symmetric structure\n",
    "        self._decoder = MLP(units, self.activation)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self._encoder(x)\n",
    "        z = softly_binarize(z, threshold=0.5)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self._decoder(z)\n",
    "        return x\n",
    "\n",
    "    def call(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentBernoulliVariationalAutoencoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    1. https://davidstutz.de/bernoulli-variational-auto-encoder-in-torch/\n",
    "    2. Information Theory, Inference, and Learning Algorithms, D. Mackay,\n",
    "       section 33.2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : [int]\n",
    "        Hidden units along the encoder. The decoder use symmetric structure.\n",
    "    activation : callable or string, optional\n",
    "        Final output activation.\n",
    "    num_samples : int, optional\n",
    "        Number of samples for Monte-Carlo integral.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, num_samples=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        self._encoder = MLP(units, name='encoder')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['activation'] = self.activation\n",
    "        config['num_samples'] = self.num_samples\n",
    "        return config\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        ambient_dim = batch_input_shape[-1]\n",
    "        units = self.units[::-1][1:] + [ambient_dim]  # symmetric structure\n",
    "        self._decoder = MLP(units, self.activation, name='decoder')\n",
    "\n",
    "        # NOTE:\n",
    "        # Since losses are computed by mean instead of sum, over all axes,\n",
    "        # we shall add a factor between the log-likelihood and the entropy.\n",
    "        # Precisely, the loss should be mean_b(sum_a(...) + sum_l(...)),\n",
    "        # where b for batch axis, a for ambient axis, and l for latent axis.\n",
    "        # However, the loss computed is mean_b(mean_a(...) + r * mean_l(...)),\n",
    "        # where r is a factor to be determined. To make the two losses\n",
    "        # proportional, we shall set r = #l / #a.\n",
    "        latent_dim = self.units[-1]\n",
    "        self._reg_factor = latent_dim / ambient_dim\n",
    "\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def encode(self, x, training=None):\n",
    "        \"\"\"Add regularizer if training.\"\"\"\n",
    "        latent_logits = self._encoder(x)\n",
    "\n",
    "        if training:\n",
    "            sampled_latent_logits = self._reparam_trick(\n",
    "                latent_logits, self.num_samples)\n",
    "            latent_entropy = self._latent_entropy(sampled_latent_logits)\n",
    "            self.add_loss(- self._reg_factor * tf.reduce_mean(latent_entropy))\n",
    "\n",
    "        else:\n",
    "            sampled_latent_logits = self._reparam_trick(latent_logits, 1)\n",
    "\n",
    "        # use the first sample as the result to return\n",
    "        z = softly_binarize(tf.nn.sigmoid(sampled_latent_logits[0]),\n",
    "                            threshold=0.5)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self._decoder(z)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        z = self.encode(x, training)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon\n",
    "\n",
    "    @staticmethod\n",
    "    def _reparam_trick(logits, num_samples):\n",
    "        \"\"\"\n",
    "        Notes\n",
    "        -----\n",
    "        s: random seed\n",
    "        p: Bernoulli probability\n",
    "        g(s, p): re-parameterization trick for Bernoulli distribution\n",
    "            E_{z ~ bernoulli(p=f(x; w))} [...(z)]\n",
    "            -> E_{s ~ uniform(0, 1)} [...(z=g(s, p=f(x;w)))],\n",
    "            where gradient(g(s, p), p) exists.\n",
    "\n",
    "        Lemma:\n",
    "            s ~ uniform(0, 1)\n",
    "            a = s / (1 - s) * p / (1 - p)\n",
    "            z = 1 if a > 1 else 0\n",
    "            => z ~ bernoulli(p)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : tensor\n",
    "            Shape [batch_size, depth].\n",
    "        num_samples : int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "            Shape [num_samples, batch_size, depth].\n",
    "        \"\"\"\n",
    "        # seed\n",
    "        eps = 1e-8\n",
    "        s = tf.random.uniform(shape=([num_samples, 1] + logits.shape[1:]),\n",
    "                              minval=eps, maxval=1-eps)  # [S, B, D]\n",
    "\n",
    "        logits = logits[tf.newaxis, ...]  # [1, B, D]\n",
    "        # employ the relation log(sigmoid(x)) - log(1 - sigmoid(x)) = x\n",
    "        sampled_logits = tf.math.log(s) - tf.math.log(1 - s) + logits\n",
    "        return sampled_logits  # [S, B, D]\n",
    "\n",
    "    @staticmethod\n",
    "    def _latent_entropy(latent_logits):\n",
    "        \"\"\"Entropy of the latent variable, per dimension.\n",
    "\n",
    "        Using Monte-Carlo integral.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_logits : tensor\n",
    "            Shape [num_samples, batch_size, depth].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "            Shape [batch_size, depth].\n",
    "        \"\"\"\n",
    "        # [S, B, D]\n",
    "        p = tf.nn.sigmoid(latent_logits)\n",
    "        log_p = log_sigmoid(latent_logits)\n",
    "        log_1mp = log_1m_sigmoid(latent_logits)\n",
    "        entropy = tf.reduce_mean(\n",
    "            -p * log_p - (1 - p) * log_1mp,  # [S, B, D]\n",
    "            axis=0)  # [B, D]\n",
    "        return entropy\n",
    "\n",
    "    def latent_entropy(self, x):\n",
    "        latent_logits = self._encoder(x)\n",
    "        sampled_latent_logits = self._reparam_trick(\n",
    "            latent_logits, self.num_samples)\n",
    "        return self._latent_entropy(sampled_latent_logits)  # [B, D]\n",
    "\n",
    "\n",
    "def log_sigmoid(x):\n",
    "    \"\"\"log(sigmoid(x)) = x - softplus(x)\"\"\"\n",
    "    return x - tf.nn.softplus(x)\n",
    "\n",
    "\n",
    "def log_1m_sigmoid(x):\n",
    "    \"\"\"log(1 - sigmoid(x)) = - softplus(x)\"\"\"\n",
    "    return - tf.nn.softplus(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [tf.keras.Input([IMAGE_SIZE[0] * IMAGE_SIZE[1]])]\n",
    "if AUTOENCODER_TYPE == 'vanilla':\n",
    "    layers += [\n",
    "        LatentBernoulliVanillaAutoencoder([64], 'sigmoid'),\n",
    "    ]\n",
    "elif AUTOENCODER_TYPE == 'variational':\n",
    "    layers += [\n",
    "        LatentBernoulliVariationalAutoencoder([64], 'sigmoid',\n",
    "                                              num_samples=10),\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError()\n",
    "if BINARIZE:\n",
    "    layers.append(SoftBinarization(0.5))\n",
    "ae = tf.keras.Sequential(layers)\n",
    "ae.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47646171875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ae, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[31.543896 31.524052 31.53149  31.965534 31.54101  31.55637  31.642353\n",
      " 31.308805 31.86382  31.670486 31.400383 31.900991 31.452078 31.507177\n",
      " 31.86161  31.555687 31.520576 31.709038 31.836185 31.489807 31.212074\n",
      " 31.33173  31.81814  31.879488 31.947119 31.338264 31.722912 31.301363\n",
      " 31.329525 31.791729 31.463463 31.574093 31.787598 31.548285 31.375692\n",
      " 31.91607  31.27531  31.2925   31.684343 31.446676 31.688192 31.567421\n",
      " 31.909021 31.463312 31.869528 31.429607 31.477207 31.585764 31.791092\n",
      " 31.462646 31.760052 31.316238 31.465881 31.797115 31.379246 31.431948\n",
      " 31.285156 31.437628 31.185686 31.842068 31.993721 31.832811 31.284138\n",
      " 30.98376  31.367378 31.889423 31.5312   31.977505 31.53477  31.258032\n",
      " 31.53238  31.772896 31.785233 31.66428  31.378485 31.450638 31.551704\n",
      " 31.812004 31.750397 31.769497 31.7119   31.47872  31.466946 31.427832\n",
      " 31.693497 31.478878 31.856358 31.35709  31.503155 31.619205 31.147844\n",
      " 31.556229 31.573456 31.657623 31.503632 31.397198 31.68028  31.392345\n",
      " 31.468012 32.01655  31.754704 31.531452 31.795841 31.660204 31.853935\n",
      " 31.744837 31.389437 31.266973 31.613071 31.646294 31.3096   31.234509\n",
      " 31.796915 31.703943 31.185966 31.467434 31.237335 31.460592 31.57494\n",
      " 31.226826 31.222614 31.065922 31.573177 31.774212 31.907883 31.60971\n",
      " 31.422039 31.551851 31.572502 31.558426 31.511051 31.573204 31.664402\n",
      " 31.618492 31.68387  31.355862 31.76455  31.70361  31.715864 31.577631\n",
      " 31.979101 31.64048  31.60632  31.496735 31.545048 31.92803  31.36549\n",
      " 31.388676 31.636889 31.460674 31.580809 31.327745 31.754517 31.385057\n",
      " 31.492292 31.388378 31.550108 31.334763 31.861588 31.49959  31.63432\n",
      " 31.550354 31.479576 31.690245 31.388546 31.355629 31.57922  31.284124\n",
      " 31.61899  31.332905 31.581043 31.611305 31.394093 31.861296 31.866488\n",
      " 31.810135 31.693361 31.741503 31.593533 31.27145  31.848188 31.219555\n",
      " 31.624092 31.230976 31.796879 31.681946 31.619644 31.364996 31.363415\n",
      " 31.318838 31.46114  32.04994  31.366344 31.739489 31.369984 31.571125\n",
      " 31.670223 31.79887  31.708546 31.421867 31.696081 31.871956 31.837738\n",
      " 31.317062 31.544506 31.782732 31.492115 31.330961 31.729544 31.394207\n",
      " 31.886908 31.635225 31.76987  31.093845 31.334143 31.357262 31.58604\n",
      " 31.447355 31.385284 31.460766 31.117271 31.173384 31.313145 31.599422\n",
      " 31.681227 31.529228 31.484753 31.451307 31.891361 31.116991 31.722763\n",
      " 31.701931 31.701332 31.258163 31.569824 31.345142 31.562874 31.379936\n",
      " 31.580345 31.779999 31.592304 31.153845 31.459316 31.500992 31.23494\n",
      " 31.656689 31.547188 31.482979 31.563236 31.635271 31.67101  31.50455\n",
      " 31.736423 31.308598 31.853481 31.07328  31.753967 31.374271 31.474207\n",
      " 31.804436 31.690573 31.303291 31.736828 31.586073 31.611734 31.450357\n",
      " 31.45762  31.425842 31.862877 31.909138 32.024094 31.378946 31.303726\n",
      " 31.707981 31.61822  31.428469 31.97537  30.95584  31.536436 31.189455\n",
      " 31.489082 31.407698 31.896923 31.091698 31.502731 31.458843 30.819626\n",
      " 31.75153  31.64178  31.426756 31.851475 31.42807  31.68834  30.952038\n",
      " 31.393675 31.455198 31.328728 31.462734 31.409382 31.120127 31.565205\n",
      " 31.602358 31.3483   31.247425 31.827261 31.710356 31.801048 31.567112\n",
      " 31.075016 31.985445 31.806274 31.435944 31.31168  31.326242 31.780361\n",
      " 31.81839  31.740932 31.17472  31.667158 31.599804 31.514006 31.210766\n",
      " 31.521626 31.353697 31.408072 31.111452 31.2253   31.59303  31.173857\n",
      " 31.445261 31.742016 31.505802 31.445446 31.271328 31.223728 31.781391\n",
      " 31.553392 31.5192   31.81674  30.994526 31.453503 31.227777 31.55756\n",
      " 31.766592 31.487782 31.69979  31.439041 31.093838 31.250614 31.723469\n",
      " 31.510239 31.713032 31.264965 31.563118 31.39737  31.845879 31.453028\n",
      " 31.59089  31.920313 31.362192 31.467579 31.299763 31.300926 31.755959\n",
      " 31.534388 31.323652 31.892288 31.444437 31.813814 31.490906 31.754086\n",
      " 31.421745 31.65338  31.686796 31.316004 31.167969 31.59358  31.37983\n",
      " 31.511984 31.631577 31.324966 31.25462  31.765131 31.148376 31.708366\n",
      " 30.956268 31.60471  31.472767 31.76582  31.369825 31.76879  31.17522\n",
      " 31.49197  31.6322   31.870075 31.329367 31.50401  31.931805 31.806923\n",
      " 30.891188 31.312187 31.474604 31.478079 31.689915 31.388046 31.162722\n",
      " 31.62297  31.683718 31.85988  31.441357 31.728405 31.796629 31.809395\n",
      " 31.460163 31.406757 31.434927 31.92444  31.86266  31.937551 31.473032\n",
      " 31.394592 31.37223  31.857668 31.6173   31.072226 31.61553  31.024\n",
      " 30.928257 31.346678 30.898235 31.661434 31.418148 31.630814 31.368923\n",
      " 31.717571 31.301126 31.785252 31.60667  31.295506 31.56153  31.193392\n",
      " 31.377253 31.575542 31.904917 31.85167  31.545195 31.667978 31.87933\n",
      " 31.341724 31.352566 31.698042 31.430992 31.52262  31.480179 31.847462\n",
      " 31.952723 31.734007 31.528053 31.247366 31.12301  31.571316 31.464977\n",
      " 31.303665 31.801796 31.384129 31.384819 32.05073  31.48055  31.832096\n",
      " 31.620043 31.732048 31.490543 31.873962 31.345251 31.715052 31.898544\n",
      " 31.776861 31.55707  31.737808 31.15118  31.222029 31.69525  31.442009\n",
      " 31.573603 31.825882 31.603165 31.549957 31.525095 31.58188  31.61607\n",
      " 31.254757 31.880817 31.786043 31.607628 31.61123  31.084314 31.693428\n",
      " 31.659698 31.583517 31.597464 31.895916 31.71604  31.543703 31.804422\n",
      " 31.07333  31.71111  31.082253 31.832512 31.920588 31.594963 31.867308\n",
      " 31.69647  31.43058  31.781975 31.642551 31.258457 31.488493 31.234695\n",
      " 31.810589 31.262148 31.186625 31.5739   31.40607  31.88809  31.181944\n",
      " 31.500767 31.264614 31.585203 30.703014 31.556522 31.615543 31.832693\n",
      " 31.416536 31.8583   31.566818 31.791786 31.621422 31.777025 31.976894\n",
      " 31.325321 31.688326 31.634945 31.150688 31.63591  31.399132 31.554695\n",
      " 31.753025 31.72946  31.366215 31.597116 31.314167 31.615381 31.788748\n",
      " 31.966509 31.362188 31.533005 31.567898 31.661184 31.941574 31.56547\n",
      " 31.675678 31.612064 31.594868 31.360186 31.402065 31.349262 31.363808\n",
      " 31.677254 31.572033 31.566345 31.596998 31.478745 31.638245 31.989319\n",
      " 31.66458  31.833395 31.817274 31.19083  31.485607 32.03704  31.465134\n",
      " 31.60522  31.61155  31.663559 31.640314 31.642008 31.929869 31.921612\n",
      " 31.78864  31.532751 31.722677 31.59055  31.849869 31.85551  31.384537\n",
      " 31.622915 31.33126  31.569363 31.33554  31.57064  31.727478 31.69661\n",
      " 31.478725 31.39339  31.99719  31.66568  31.447948 31.560314 31.44344\n",
      " 31.875074 31.714987 31.763111 31.329777 31.079071 31.415112 31.309729\n",
      " 31.108727 31.626505 31.778265 31.65963  31.18243  31.615921 31.117767\n",
      " 31.447552 31.627697 31.396671 31.152948 31.817457 31.728628 31.409817\n",
      " 31.697155 31.536083 31.32426  31.316244 31.980526 31.455275 31.73693\n",
      " 31.922834 31.857384 31.525682 31.62196  31.605604 31.29018  31.542849\n",
      " 31.797298 31.27266  31.657122 31.77915  32.017353 31.399853 31.786074\n",
      " 31.642324 31.420925 31.542522 31.509075 31.627012 31.281229 31.541773\n",
      " 31.44285  31.61113  31.748827 31.631416 31.458958 31.745823 31.724098\n",
      " 31.37729  31.47089  31.54341  31.429867 31.165379 31.575745 31.65245\n",
      " 31.219624 31.128658 31.84549  31.4185   31.7982   31.106838 31.83293\n",
      " 31.54989  31.691477 31.126554 31.731981 31.418125 31.53482  31.903402\n",
      " 31.813599 31.353228 31.74457  31.295122 31.425873 31.711407 31.293114\n",
      " 31.422668 31.539425 31.143589 31.873535 31.409334 31.95629  31.457521\n",
      " 31.657364 31.389404 31.436213 31.64933  31.709568 31.726263 31.363756\n",
      " 31.394932 31.774086 31.357182 31.797234 31.865742 31.166008 31.232536\n",
      " 31.656593 31.274565 31.127811 31.338945 31.692612 31.3639   31.693695\n",
      " 31.17664  31.339731 31.577118 31.528141 31.820898 31.599226 31.258202\n",
      " 31.605635 31.008427 31.512894 31.097427 31.52058  31.062202 31.402203\n",
      " 31.142902 31.710022 31.054356 31.876343 31.189682 31.512096 31.454855\n",
      " 31.56916  30.953674 31.686298 31.099838 31.637516 31.925156 31.579369\n",
      " 31.559727 31.629568 31.220158 31.381317 31.523369 31.828175 31.476597\n",
      " 30.88947  31.498432 31.447817 31.220272 31.79859  31.54799  31.781923\n",
      " 31.10992  31.2401   31.814096 31.392332 31.105099 31.485287 31.597963\n",
      " 31.641596 31.239204 31.2425   31.61215  31.514967 31.602453 31.287235\n",
      " 31.51836  31.846123 31.780952 31.836605 30.929705 31.814594 31.726952\n",
      " 31.746918 31.393314 31.939003 31.143415 31.360256 31.211376 31.433044\n",
      " 31.642395 31.498844 31.358532 31.48655  31.5088   31.741598 31.349564\n",
      " 31.448444 31.52702  31.308512 31.362995 31.368809 31.688337 31.595568\n",
      " 31.293379 31.005592 31.523045 31.561832 31.904512 31.417984 31.46732\n",
      " 31.504417 31.618195 31.279018 31.462938 31.372684 31.517897 31.49291\n",
      " 31.585075 31.49243  31.729729 31.799496 31.58077  31.7562   31.226322\n",
      " 31.219666 31.55868  31.47154  31.292618 31.604702 31.387886 31.697357\n",
      " 31.81028  31.463223 31.796692 31.682316 31.495037 31.750475 31.582024\n",
      " 31.607716 31.257233 31.317944 31.371725 31.502686 31.256962 31.389915\n",
      " 31.31704  31.56685  31.170033 31.447485 31.114563 31.498676 31.577766\n",
      " 31.5517   31.29048  31.571661 31.378178 31.445358 31.181593 31.241282\n",
      " 31.194176 31.631004 31.783752 31.926027 31.516304 31.454895 31.181793\n",
      " 31.791903 31.344296 31.676584 31.898687 31.225052 31.409756 31.280355\n",
      " 31.330893 31.665764 31.296703 31.607351 31.68287  31.532143 31.467995\n",
      " 31.676968 31.373997 31.858898 31.469814 31.464312 31.281052 31.538109\n",
      " 31.550568 31.410345 31.898687 31.73762  31.460297 31.69576  31.012169\n",
      " 31.276598 31.51614  31.552704 31.68326  31.766813 31.327068 31.554459\n",
      " 31.389072 31.401106 31.788511 31.467224 31.372385 31.55983  31.198997\n",
      " 31.552605 31.448874 31.58038  31.62175  31.330462 31.714954 31.570892\n",
      " 31.557575 31.617125 31.854845 31.642197 31.73486  31.858328 31.303015\n",
      " 31.47372  31.312286 31.038336 31.335182 31.646963 31.571339 31.393091\n",
      " 31.496243 31.45094  31.710215 31.508764 31.83254  31.682522 31.742107\n",
      " 31.519455 31.723866 31.153519 31.954222 31.535122 31.389265 30.890802\n",
      " 31.667711 31.479294 31.806492 31.641947 31.474121 31.512491 31.89489\n",
      " 31.568218 31.394413 31.561792 31.631044 31.48325  31.451338 31.397783\n",
      " 31.594604 31.276102 31.97891  31.618862 31.58541  31.704842 31.438347\n",
      " 31.828396 31.580647 31.752905 31.432318 31.519463 31.883179 31.092201\n",
      " 31.476728 31.199028 31.424026 31.565569 31.64847  31.912119 31.326845\n",
      " 31.568174 31.341614 31.739323 31.281319 31.50557  31.698631 31.149767\n",
      " 31.338066 31.191412 31.441523 31.295298 31.27335  31.411373 31.997677\n",
      " 31.65807  31.29776  31.689072 31.431374 31.712421 31.666622], shape=(1000,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "if AUTOENCODER_TYPE == 'variational':\n",
    "    print(tf.reduce_sum(ae.layers[0].latent_entropy(x_train[:1000]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14063/14063 [==============================] - 88s 6ms/step - loss: 0.0625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff82e9f2050>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train[:NUM_DATA], x_train[:NUM_DATA]))\n",
    "ds = ds.shuffle(10000).repeat(30).batch(128)\n",
    "ae.fit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9384083333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ae, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[24.115875 23.449432 24.686804 25.713158 26.370564 24.737907 24.313774\n",
      " 23.923756 26.921467 25.201267 24.497602 24.8684   19.681847 23.896519\n",
      " 25.995813 24.782166 27.07545  24.605915 28.056368 26.23744  20.100037\n",
      " 21.993702 27.763973 25.671062 26.260931 21.10859  27.865786 21.467285\n",
      " 20.319391 26.330294 24.35022  22.199757 25.53824  24.434954 22.852627\n",
      " 25.563826 23.516817 22.747967 24.421787 26.06112  26.24363  26.158676\n",
      " 27.798985 26.474957 27.848686 25.855877 27.258793 24.138905 27.198025\n",
      " 24.395645 26.659698 19.643288 23.43298  26.087477 23.673374 24.426994\n",
      " 20.330238 26.128368 22.321009 24.999609 19.720984 26.505764 22.941936\n",
      " 21.254086 24.284264 25.678848 24.701141 26.678791 22.817497 22.092617\n",
      " 23.734932 24.93187  26.456257 25.266794 25.315685 23.578976 25.272968\n",
      " 25.973007 25.515083 25.663837 24.7832   23.09982  22.075518 24.729897\n",
      " 25.607943 25.264973 22.566319 26.18824  22.075565 23.621403 21.677147\n",
      " 22.908035 27.433706 26.778246 25.514889 22.735844 26.073597 24.201633\n",
      " 26.705032 27.31157  26.26529  24.511677 25.329672 25.702309 26.200909\n",
      " 24.984037 26.095654 22.35684  22.820156 25.872334 23.611782 20.425581\n",
      " 27.619701 25.845863 19.596485 26.263739 25.761112 24.11007  24.893372\n",
      " 20.812498 20.329601 18.570677 24.172497 23.106094 27.887182 26.124338\n",
      " 24.478106 24.255703 25.92857  24.68879  26.697784 24.848824 26.099823\n",
      " 26.314749 25.407825 21.362392 25.517756 25.16458  24.644875 25.499437\n",
      " 27.115507 25.105064 28.058308 24.101227 27.319336 25.893648 23.739166\n",
      " 24.796486 27.95449  23.144943 25.39782  23.960608 25.93246  26.712368\n",
      " 25.762955 24.700253 24.318466 22.358994 25.730953 23.380335 26.342894\n",
      " 23.271187 26.176262 25.612465 19.202518 22.75841  26.741589 26.117968\n",
      " 26.353859 22.590132 24.618132 23.972574 24.121414 25.580074 27.128086\n",
      " 24.488012 25.532246 25.813396 24.535942 21.57211  26.52343  22.735352\n",
      " 25.105286 25.949509 26.165611 22.35914  25.63928  21.826344 24.190887\n",
      " 22.561935 23.22863  27.788807 23.883835 23.454453 23.437843 25.525364\n",
      " 23.242588 25.76733  23.681686 25.373783 24.69526  25.485878 26.517094\n",
      " 23.336447 24.872696 25.54755  21.802338 25.365715 25.79771  19.735098\n",
      " 26.279999 25.342667 26.517895 21.583216 21.640491 23.119154 25.1658\n",
      " 26.452206 24.335232 23.078384 20.22957  21.728481 20.3466   24.704372\n",
      " 25.327175 24.075052 25.520771 26.11863  25.259296 19.620255 26.837326\n",
      " 25.24776  28.631771 22.492    23.885986 22.218876 24.239113 24.585546\n",
      " 25.743336 23.985466 27.648155 22.084682 21.91602  24.290894 23.096039\n",
      " 23.659878 21.065643 26.620659 24.004314 21.907288 25.211384 26.190182\n",
      " 23.223766 17.613731 25.830189 22.118591 26.078228 25.897682 27.198093\n",
      " 23.871666 26.089445 23.744715 26.966871 24.017092 25.977886 24.02296\n",
      " 24.795088 26.467392 25.00158  25.949034 28.552847 26.432697 23.33339\n",
      " 24.337786 25.507584 25.528767 27.04156  20.382872 24.559557 23.939983\n",
      " 27.990265 21.423859 24.329922 20.00483  21.637207 26.074152 22.487696\n",
      " 23.685102 26.093103 26.198545 26.40561  24.068802 27.027794 21.206835\n",
      " 17.817331 25.162804 24.175167 25.216867 21.241512 23.317482 26.618546\n",
      " 25.223795 24.765839 21.674374 26.738388 25.65937  24.472588 25.231438\n",
      " 20.924252 27.33738  27.5169   23.06242  24.876396 26.78531  26.659033\n",
      " 25.549328 24.938755 20.644001 26.004745 25.987658 22.777021 23.688694\n",
      " 25.28241  21.682926 27.40866  19.91988  23.360481 21.969553 23.926918\n",
      " 23.96505  24.999577 23.643341 24.42199  23.598022 25.264427 26.328747\n",
      " 27.068933 23.430075 26.95047  20.400963 25.33157  20.577908 27.186268\n",
      " 25.381153 25.971634 25.496914 26.410828 18.855963 24.689255 23.726606\n",
      " 25.692348 25.016933 23.01008  25.472164 25.663485 25.665394 26.855257\n",
      " 25.048004 26.654236 23.44827  20.425467 24.454988 25.861757 26.277582\n",
      " 27.880987 21.771011 26.982983 21.871702 26.992382 26.941574 25.819485\n",
      " 22.804234 27.91304  26.484058 24.74963  19.585007 26.210527 23.448477\n",
      " 21.457458 26.954617 23.763672 23.535812 24.86045  24.965248 26.699932\n",
      " 17.749115 22.836332 25.148106 27.166267 25.92974  25.702503 20.326866\n",
      " 23.344315 25.289474 26.679407 23.367023 27.942585 25.957596 27.171982\n",
      " 21.15968  22.165369 26.313774 26.599554 22.586126 24.870762 22.51186\n",
      " 24.523838 23.62822  27.333374 27.2994   25.01528  24.770016 29.224295\n",
      " 25.629013 25.699226 22.841671 26.19666  26.26167  29.323595 25.46419\n",
      " 26.62714  23.7243   26.671276 25.99135  21.840714 24.88212  19.786474\n",
      " 20.788591 24.703718 18.719025 27.744705 24.469128 23.789972 23.234547\n",
      " 26.08864  22.459816 28.225319 25.327915 25.055826 24.157633 18.586926\n",
      " 26.05512  27.67513  26.273466 26.207891 22.753218 26.60104  25.950186\n",
      " 22.04286  24.180357 26.276085 24.632273 22.58347  23.414604 27.781837\n",
      " 26.714106 28.384834 21.478237 20.923752 26.292908 25.349564 23.530943\n",
      " 19.159157 26.455032 19.998596 23.654367 27.476889 20.645676 26.032555\n",
      " 24.894405 27.22335  24.80402  27.614635 20.648851 26.587627 25.933424\n",
      " 27.767399 23.152065 23.530296 25.756618 24.095581 25.6596   28.0469\n",
      " 21.073658 27.36855  25.466858 22.829708 26.275799 24.29921  24.46176\n",
      " 23.99984  24.754124 25.656479 23.759392 22.544628 23.8708   25.85403\n",
      " 24.30927  25.962233 25.51823  26.34022  26.824783 27.391176 24.39617\n",
      " 21.081255 24.015022 21.947147 25.723738 25.31609  24.003246 26.439232\n",
      " 23.90551  25.372473 24.730194 26.722376 20.154999 23.86583  23.700893\n",
      " 26.99091  20.932415 25.197403 21.774858 22.9314   26.285408 21.590229\n",
      " 23.953568 19.838053 24.271414 21.701221 25.459202 23.56345  26.278797\n",
      " 24.543415 24.601873 23.420998 23.535215 26.72163  23.988146 26.83879\n",
      " 23.496824 27.185028 24.491682 22.925049 25.493391 23.511795 24.622452\n",
      " 24.240158 25.220173 22.349514 23.152876 26.492432 22.363035 25.67241\n",
      " 27.100811 23.364399 22.548767 25.349325 21.623121 27.217815 23.296848\n",
      " 26.595648 23.042156 24.198153 22.050318 25.42139  23.871338 26.482344\n",
      " 24.42194  23.993124 24.05016  20.996712 24.391098 26.205776 26.300892\n",
      " 27.778566 25.24844  26.06279  20.953976 25.948729 25.306879 25.53403\n",
      " 23.940937 24.485138 26.15398  28.568996 26.129242 27.200422 25.578869\n",
      " 27.304178 24.647757 28.47081  21.59998  25.271774 25.63961  24.9511\n",
      " 26.022512 22.33022  23.600628 23.229322 25.273754 26.991404 25.971119\n",
      " 25.541441 20.626537 28.451824 26.359388 19.393202 26.206024 23.250534\n",
      " 25.748978 22.972454 25.478106 23.369152 26.789165 23.869127 24.032436\n",
      " 23.096909 23.78979  25.973    23.566051 22.200378 26.095444 22.22583\n",
      " 23.31724  25.664764 25.005608 21.90997  26.45748  28.087181 23.917606\n",
      " 26.421192 25.197268 20.098787 21.44638  27.982088 23.843124 27.56615\n",
      " 25.422848 26.820375 22.229313 20.986616 25.40499  22.250107 23.95948\n",
      " 26.865969 26.43576  26.339405 27.305569 26.590963 23.186161 23.996408\n",
      " 22.25815  25.228783 25.934155 25.315575 23.271362 23.178835 22.955128\n",
      " 22.274603 24.568983 27.093441 26.53938  24.543362 24.424461 24.157103\n",
      " 22.071222 23.643919 22.738697 23.068377 19.60607  22.476173 25.27046\n",
      " 24.281437 20.947441 27.755688 23.235611 26.59977  22.06274  26.655796\n",
      " 24.417912 28.04044  22.40604  26.829433 22.868425 22.586403 25.341984\n",
      " 26.502384 26.36184  25.103006 20.786674 25.887619 25.359848 24.170803\n",
      " 23.338959 26.811947 22.771366 26.204992 21.56098  26.751133 22.652428\n",
      " 27.016354 20.732075 22.2202   23.37725  23.480122 23.987858 24.868834\n",
      " 26.048494 26.845818 20.972984 25.68217  25.131403 20.880526 20.77216\n",
      " 26.142492 22.251682 24.289927 23.678474 25.92147  21.852104 26.128336\n",
      " 21.354774 21.762789 22.63195  22.030579 24.329382 26.590252 26.069038\n",
      " 28.071447 21.997105 25.815205 22.080666 21.174358 21.669708 22.477797\n",
      " 22.242832 28.762615 22.782299 27.028118 21.516367 25.775991 26.270601\n",
      " 25.920656 18.278118 27.194138 17.912935 26.593353 25.117527 26.386284\n",
      " 23.170341 25.943363 23.802284 23.619305 23.45727  28.542673 24.89545\n",
      " 22.43335  23.375502 24.61021  23.029024 25.96494  24.234068 26.347595\n",
      " 23.203165 21.841036 24.972189 24.205055 22.913784 25.763176 23.78886\n",
      " 27.25915  23.075039 23.178825 25.159302 21.124166 26.446575 23.726639\n",
      " 24.249962 29.250013 24.61616  25.254179 19.383179 28.195059 25.738731\n",
      " 27.588581 22.630241 25.968554 21.17187  26.28713  23.512962 21.881205\n",
      " 24.236313 23.89039  25.349346 21.947594 24.327015 28.855604 24.162523\n",
      " 25.03964  21.295364 25.616241 23.910023 25.201834 26.270905 23.892265\n",
      " 24.197823 25.68693  25.287971 26.00505  26.044538 22.20552  22.982128\n",
      " 26.241808 25.10724  19.222788 23.402912 24.232525 24.740297 24.654795\n",
      " 23.523298 22.836819 23.424494 26.99636  24.802567 26.08894  20.259686\n",
      " 20.551449 26.192474 20.905289 23.672285 23.809214 22.679016 25.732265\n",
      " 25.020031 26.442835 25.30042  26.60661  24.542759 26.314764 25.103333\n",
      " 23.578856 21.793606 22.92937  23.085537 23.306698 21.771511 24.583881\n",
      " 24.223415 24.211193 20.21917  24.978573 22.74702  24.429716 25.482555\n",
      " 25.377176 22.444057 26.474503 21.467371 26.836485 20.966671 24.15063\n",
      " 22.925207 22.152695 26.101738 27.510212 24.433033 22.858978 22.517397\n",
      " 26.570923 22.916796 24.962433 26.21936  18.34499  20.675901 22.071049\n",
      " 19.982046 26.328333 24.2724   27.730272 26.544563 24.442629 24.196957\n",
      " 23.045284 19.918554 26.600466 24.08675  22.054527 22.918133 26.163992\n",
      " 21.725721 24.190521 26.21936  27.630306 20.102709 24.891073 21.117924\n",
      " 21.163403 23.92313  27.900227 27.495426 28.314217 24.150911 24.207558\n",
      " 23.306812 22.980736 24.342573 24.751492 20.332415 20.923077 20.37169\n",
      " 24.22349  21.772633 25.968323 23.214378 24.794773 25.790974 24.455956\n",
      " 21.71936  25.410063 25.402115 25.274809 24.94588  27.982025 22.210352\n",
      " 20.719381 22.955368 22.552687 21.833864 25.183714 25.362993 24.140253\n",
      " 22.265354 24.789478 25.155884 22.81356  24.096699 26.015701 25.945633\n",
      " 26.298416 26.218168 24.118574 27.42828  21.47477  25.255482 21.4627\n",
      " 26.262623 23.21924  26.2122   25.381525 21.90228  25.748976 25.027046\n",
      " 25.20633  26.192371 24.567844 25.317198 23.050035 25.465082 24.568459\n",
      " 23.460129 22.395155 27.05883  23.492147 24.020851 23.72166  23.453924\n",
      " 25.210867 24.658394 27.143017 22.014275 24.068039 25.833208 23.907413\n",
      " 24.68972  24.892561 20.277416 25.560274 24.634933 28.343067 22.69788\n",
      " 26.880226 20.541483 26.025162 25.608616 22.426456 26.06255  21.53112\n",
      " 24.7922   24.452536 21.35125  22.10772  25.036491 23.496414 28.29865\n",
      " 21.293283 22.042349 25.414028 21.885914 25.89478  23.315113], shape=(1000,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "if AUTOENCODER_TYPE == 'variational':\n",
    "    print(tf.reduce_sum(ae.layers[0].latent_entropy(x_train[:1000]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
