{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from mnist import load_mnist, evaluate\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), _ = load_mnist(image_size=(16, 16), binarize=True)\n",
    "x_train = (x_train + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(x, threshold):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tensor\n",
    "    threshold : float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The same shape and dtype as x\n",
    "    \"\"\"\n",
    "    y = tf.where(x > threshold, 1, 0)\n",
    "    y = tf.cast(y, x.dtype)\n",
    "    return y\n",
    "\n",
    "\n",
    "def softly_binarize(x, threshold):\n",
    "\n",
    "    def identity(dy):\n",
    "        return dy\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def fn(x):\n",
    "        y = binarize(x, threshold)\n",
    "        return y, identity\n",
    "\n",
    "    return fn(x)\n",
    "\n",
    "\n",
    "class SoftBinarization(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, threshold, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def call(self, x):\n",
    "        return softly_binarize(x, self.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        layers = [tf.keras.layers.Dense(n, 'relu') for n in self.units[:-1]]\n",
    "        layers.append(tf.keras.layers.Dense(self.units[-1], self.activation))\n",
    "        self._ffn = tf.keras.Sequential(layers)\n",
    "        self._ffn.build(batch_input_shape)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self._ffn(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class LatentBernoulliVanillaAutoencoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    1. https://davidstutz.de/bernoulli-variational-auto-encoder-in-torch/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "        self._encoder = FeedForwardNetwork(units, 'sigmoid')\n",
    "        self._soft_binarization = SoftBinarization(0.5)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        ambient_dim = batch_input_shape[-1]\n",
    "        units = self.units[::-1][1:] + [ambient_dim]  # symmetric structure\n",
    "        self._decoder = FeedForwardNetwork(units, self.activation)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self._encoder(x)\n",
    "        z = self._soft_binarization(z)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, training=None):\n",
    "        x = self._decoder(z)\n",
    "        if not training:\n",
    "            x = self._soft_binarization(x)\n",
    "        return x\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        return self.decode(self.encode(x), training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentBernoulliVariationalAutoencoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "        self._encoder = FeedForwardNetwork(units)\n",
    "        self._soft_binarization = SoftBinarization(0.5)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        ambient_dim = batch_input_shape[-1]\n",
    "        units = self.units[::-1][1:] + [ambient_dim]  # symmetric structure\n",
    "        self._decoder = FeedForwardNetwork(units, self.activation)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        logits = self._encoder(x)\n",
    "        z = self._reparam_trick(logits)\n",
    "        x_recon = self._decoder(z)\n",
    "        if training:\n",
    "            self.add_loss(self._kl_div(z, logits))\n",
    "        else:\n",
    "            x_recon = self._soft_binarization(x_recon)\n",
    "        return x_recon\n",
    "\n",
    "    def _reparam_trick(self, logits):\n",
    "        eps = 1e-8\n",
    "        s = tf.random.uniform(logits.shape[1:], minval=eps, maxval=1-eps)\n",
    "        s = s[tf.newaxis, ...]\n",
    "        z = tf.nn.sigmoid(\n",
    "            tf.math.log(s) - tf.math.log(1 - s)\n",
    "            + logits)\n",
    "        return z\n",
    "\n",
    "    def _kl_div(self, z, logits):\n",
    "        \"\"\"KL-divergence between Q-distribution and latent prior.\"\"\"\n",
    "        log_z = -tf.nn.softplus(-logits)\n",
    "        log_1mz = log_z - logits\n",
    "        return tf.reduce_mean(\n",
    "            tf.where(z > 0.5, log_z, log_1mz)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = tf.keras.Sequential([\n",
    "    # LatentBernoulliVanillaAutoencoder([128], 'sigmoid'),\n",
    "    LatentBernoulliVariationalAutoencoder([128], 'sigmoid'),\n",
    "])\n",
    "ae.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7813/7813 [==============================] - 38s 5ms/step - loss: -0.3088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa098294210>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((x_train[:10000], x_train[:10000]))\n",
    "ds = ds.shuffle(10000).repeat(100).batch(128)\n",
    "# ae.fit(x_train[:1000], x_train[:1000], epochs=1)\n",
    "ae.fit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.953671875"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ae, x_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
