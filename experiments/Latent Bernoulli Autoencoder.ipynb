{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from mnist import load_mnist, evaluate\n",
    "from hopfield.utils import soft_step\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global configurations\n",
    "\n",
    "IMAGE_SIZE = (16, 16)\n",
    "BINARIZE = True\n",
    "AUTOENCODER_TYPE = ('vanilla', 'variational')[1]\n",
    "NUM_DATA = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), _ = load_mnist(image_size=IMAGE_SIZE, binarize=BINARIZE)\n",
    "x_train = (x_train + 1) / 2  # x \\in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_binary(x):\n",
    "    return soft_step(x, 0.5, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    \"\"\"Multi-layer perceptron (MLP).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : [int]\n",
    "    activation : callable or string, optional\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['activation'] = self.activation\n",
    "        return config\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        layers = [tf.keras.layers.Dense(n, 'relu') for n in self.units[:-1]]\n",
    "        layers.append(tf.keras.layers.Dense(self.units[-1], self.activation))\n",
    "        self._ffn = tf.keras.Sequential(layers)\n",
    "        self._ffn.build(batch_input_shape)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self._ffn(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentBernoulliVanillaAutoencoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : [int]\n",
    "        Hidden units along the encoder. The decoder use symmetric structure.\n",
    "    activation : callable or string, optional\n",
    "        Final output activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "        self._encoder = MLP(units, soft_binary)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['activation'] = self.activation\n",
    "        return config\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        ambient_dim = batch_input_shape[-1]\n",
    "        units = self.units[::-1][1:] + [ambient_dim]  # symmetric structure\n",
    "        self._decoder = MLP(units, self.activation)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self._encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self._decoder(z)\n",
    "        return x\n",
    "\n",
    "    def call(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentBernoulliVariationalAutoencoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    1. https://davidstutz.de/bernoulli-variational-auto-encoder-in-torch/\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : [int]\n",
    "        Hidden units along the encoder. The decoder use symmetric structure.\n",
    "    activation : callable or string, optional\n",
    "        Final output activation.\n",
    "    num_samples : int, optional\n",
    "        Number of samples for Monte-Carlo integral.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, num_samples=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        self._encoder = MLP(units, name='encoder')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['activation'] = self.activation\n",
    "        config['num_samples'] = self.num_samples\n",
    "        return config\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        ambient_dim = batch_input_shape[-1]\n",
    "        units = self.units[::-1][1:] + [ambient_dim]  # symmetric structure\n",
    "        self._decoder = MLP(units, self.activation, name='decoder')\n",
    "\n",
    "        # NOTE:\n",
    "        # Since losses are computed by mean instead of sum, over all axes,\n",
    "        # we shall add a factor between the log-likelihood and the entropy.\n",
    "        # Precisely, the loss should be mean_b(sum_a(...) + sum_l(...)),\n",
    "        # where b for batch axis, a for ambient axis, and l for latent axis.\n",
    "        # However, the loss computed is mean_b(mean_a(...) + r * mean_l(...)),\n",
    "        # where r is a factor to be determined. To make the two losses\n",
    "        # proportional, we shall set r = #l / #a.\n",
    "        latent_dim = self.units[-1]\n",
    "        self._reg_factor = latent_dim / ambient_dim\n",
    "\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def encode(self, x, training=None):\n",
    "        \"\"\"Add regularizer if training.\"\"\"\n",
    "        latent_logits = self._encoder(x)\n",
    "\n",
    "        if training:\n",
    "            sampled_latent_logits = self._reparam_trick(\n",
    "                latent_logits, self.num_samples)\n",
    "            latent_entropy = self._latent_entropy(sampled_latent_logits)\n",
    "            self.add_loss(- self._reg_factor * tf.reduce_mean(latent_entropy))\n",
    "\n",
    "        else:\n",
    "            sampled_latent_logits = self._reparam_trick(latent_logits, 1)\n",
    "\n",
    "        # use the first sample as the result to return\n",
    "        z = soft_binary(sampled_latent_logits[0])\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self._decoder(z)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        z = self.encode(x, training)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon\n",
    "\n",
    "    @staticmethod\n",
    "    def _reparam_trick(logits, num_samples):\n",
    "        \"\"\"\n",
    "        Notes\n",
    "        -----\n",
    "        s: random seed\n",
    "        p: Bernoulli probability\n",
    "        g(s, p): re-parameterization trick for Bernoulli distribution\n",
    "            E_{z ~ bernoulli(p=f(x; w))} [...(z)]\n",
    "            -> E_{s ~ uniform(0, 1)} [...(z=g(s, p=f(x;w)))],\n",
    "            where gradient(g(s, p), p) exists.\n",
    "\n",
    "        Lemma:\n",
    "            s ~ uniform(0, 1)\n",
    "            a = s / (1 - s) * p / (1 - p)\n",
    "            z = 1 if a > 1 else 0\n",
    "            => z ~ bernoulli(p)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : tensor\n",
    "            Shape [batch_size, depth].\n",
    "        num_samples : int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "            Shape [num_samples, batch_size, depth].\n",
    "        \"\"\"\n",
    "        # seed\n",
    "        eps = 1e-8\n",
    "        s = tf.random.uniform(shape=([num_samples, 1] + logits.shape[1:]),\n",
    "                              minval=eps, maxval=1-eps)  # [S, B, D]\n",
    "\n",
    "        logits = logits[tf.newaxis, ...]  # [1, B, D]\n",
    "        # employ the relation log(sigmoid(x)) - log(1 - sigmoid(x)) = x\n",
    "        sampled_logits = tf.math.log(s) - tf.math.log(1 - s) + logits\n",
    "        return sampled_logits  # [S, B, D]\n",
    "\n",
    "    @staticmethod\n",
    "    def _latent_entropy(latent_logits):\n",
    "        \"\"\"Entropy of the latent variable, per dimension.\n",
    "\n",
    "        Using Monte-Carlo integral.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_logits : tensor\n",
    "            Shape [num_samples, batch_size, depth].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "            Shape [batch_size, depth].\n",
    "        \"\"\"\n",
    "        # [S, B, D]\n",
    "        p = tf.nn.sigmoid(latent_logits)\n",
    "        log_p = log_sigmoid(latent_logits)\n",
    "        log_1mp = log_1m_sigmoid(latent_logits)\n",
    "        entropy = tf.reduce_mean(\n",
    "            -p * log_p - (1 - p) * log_1mp,  # [S, B, D]\n",
    "            axis=0)  # [B, D]\n",
    "        return entropy\n",
    "\n",
    "    def latent_entropy(self, x):\n",
    "        latent_logits = self._encoder(x)\n",
    "        sampled_latent_logits = self._reparam_trick(\n",
    "            latent_logits, self.num_samples)\n",
    "        return self._latent_entropy(sampled_latent_logits)  # [B, D]\n",
    "\n",
    "\n",
    "def log_sigmoid(x):\n",
    "    \"\"\"log(sigmoid(x)) = x - softplus(x)\"\"\"\n",
    "    return x - tf.nn.softplus(x)\n",
    "\n",
    "\n",
    "def log_1m_sigmoid(x):\n",
    "    \"\"\"log(1 - sigmoid(x)) = - softplus(x)\"\"\"\n",
    "    return - tf.nn.softplus(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftBinarization(tf.keras.layers.Layer):\n",
    "    \"\"\"For using in tf.keras.Sequential.\n",
    "\n",
    "    If in training phase, then do nothing. Otherwise, make soft binarization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float\n",
    "    from_logits : bool, optional\n",
    "        If true, then softly binarize sigmoid(x) instead of x.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, from_logits=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['threshold'] = self.threshold\n",
    "        config['from_logits'] = self.from_logits\n",
    "        return config\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if self.from_logits:\n",
    "            x = tf.nn.sigmoid(x)\n",
    "        if training:\n",
    "            return x\n",
    "        return soft_binary(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [tf.keras.Input([IMAGE_SIZE[0] * IMAGE_SIZE[1]])]\n",
    "if AUTOENCODER_TYPE == 'vanilla':\n",
    "    layers += [\n",
    "        LatentBernoulliVanillaAutoencoder([64], 'sigmoid'),\n",
    "    ]\n",
    "elif AUTOENCODER_TYPE == 'variational':\n",
    "    layers += [\n",
    "        LatentBernoulliVariationalAutoencoder([64], 'sigmoid',\n",
    "                                              num_samples=10),\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError()\n",
    "if BINARIZE:\n",
    "    layers.append(SoftBinarization())\n",
    "ae = tf.keras.Sequential(layers)\n",
    "ae.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4813040364583333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ae, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[31.543896 31.524052 31.53149  31.965534 31.54101  31.55637  31.642353\n",
      " 31.308805 31.86382  31.670486 31.400383 31.900991 31.452078 31.507177\n",
      " 31.86161  31.555687 31.520576 31.709038 31.836185 31.489807 31.212074\n",
      " 31.33173  31.81814  31.879488 31.947119 31.338264 31.722912 31.301363\n",
      " 31.329525 31.791729 31.463463 31.574093 31.787598 31.548285 31.375692\n",
      " 31.91607  31.27531  31.2925   31.684343 31.446676 31.688192 31.567421\n",
      " 31.909021 31.463312 31.869528 31.429607 31.477207 31.585764 31.791092\n",
      " 31.462646 31.760052 31.316238 31.465881 31.797115 31.379246 31.431948\n",
      " 31.285156 31.437628 31.185686 31.842068 31.993721 31.832811 31.284138\n",
      " 30.98376  31.367378 31.889423 31.5312   31.977505 31.53477  31.258032\n",
      " 31.53238  31.772896 31.785233 31.66428  31.378485 31.450638 31.551704\n",
      " 31.812004 31.750397 31.769497 31.7119   31.47872  31.466946 31.427832\n",
      " 31.693497 31.478878 31.856358 31.35709  31.503155 31.619205 31.147844\n",
      " 31.556229 31.573456 31.657623 31.503632 31.397198 31.68028  31.392345\n",
      " 31.468012 32.01655  31.754704 31.531452 31.795841 31.660204 31.853935\n",
      " 31.744837 31.389437 31.266973 31.613071 31.646294 31.3096   31.234509\n",
      " 31.796915 31.703943 31.185966 31.467434 31.237335 31.460592 31.57494\n",
      " 31.226826 31.222614 31.065922 31.573177 31.774212 31.907883 31.60971\n",
      " 31.422039 31.551851 31.572502 31.558426 31.511051 31.573204 31.664402\n",
      " 31.618492 31.68387  31.355862 31.76455  31.70361  31.715864 31.577631\n",
      " 31.979101 31.64048  31.60632  31.496735 31.545048 31.92803  31.36549\n",
      " 31.388676 31.636889 31.460674 31.580809 31.327745 31.754517 31.385057\n",
      " 31.492292 31.388378 31.550108 31.334763 31.861588 31.49959  31.63432\n",
      " 31.550354 31.479576 31.690245 31.388546 31.355629 31.57922  31.284124\n",
      " 31.61899  31.332905 31.581043 31.611305 31.394093 31.861296 31.866488\n",
      " 31.810135 31.693361 31.741503 31.593533 31.27145  31.848188 31.219555\n",
      " 31.624092 31.230976 31.796879 31.681946 31.619644 31.364996 31.363415\n",
      " 31.318838 31.46114  32.04994  31.366344 31.739489 31.369984 31.571125\n",
      " 31.670223 31.79887  31.708546 31.421867 31.696081 31.871956 31.837738\n",
      " 31.317062 31.544506 31.782732 31.492115 31.330961 31.729544 31.394207\n",
      " 31.886908 31.635225 31.76987  31.093845 31.334143 31.357262 31.58604\n",
      " 31.447355 31.385284 31.460766 31.117271 31.173384 31.313145 31.599422\n",
      " 31.681227 31.529228 31.484753 31.451307 31.891361 31.116991 31.722763\n",
      " 31.701931 31.701332 31.258163 31.569824 31.345142 31.562874 31.379936\n",
      " 31.580345 31.779999 31.592304 31.153845 31.459316 31.500992 31.23494\n",
      " 31.656689 31.547188 31.482979 31.563236 31.635271 31.67101  31.50455\n",
      " 31.736423 31.308598 31.853481 31.07328  31.753967 31.374271 31.474207\n",
      " 31.804436 31.690573 31.303291 31.736828 31.586073 31.611734 31.450357\n",
      " 31.45762  31.425842 31.862877 31.909138 32.024094 31.378946 31.303726\n",
      " 31.707981 31.61822  31.428469 31.97537  30.95584  31.536436 31.189455\n",
      " 31.489082 31.407698 31.896923 31.091698 31.502731 31.458843 30.819626\n",
      " 31.75153  31.64178  31.426756 31.851475 31.42807  31.68834  30.952038\n",
      " 31.393675 31.455198 31.328728 31.462734 31.409382 31.120127 31.565205\n",
      " 31.602358 31.3483   31.247425 31.827261 31.710356 31.801048 31.567112\n",
      " 31.075016 31.985445 31.806274 31.435944 31.31168  31.326242 31.780361\n",
      " 31.81839  31.740932 31.17472  31.667158 31.599804 31.514006 31.210766\n",
      " 31.521626 31.353697 31.408072 31.111452 31.2253   31.59303  31.173857\n",
      " 31.445261 31.742016 31.505802 31.445446 31.271328 31.223728 31.781391\n",
      " 31.553392 31.5192   31.81674  30.994526 31.453503 31.227777 31.55756\n",
      " 31.766592 31.487782 31.69979  31.439041 31.093838 31.250614 31.723469\n",
      " 31.510239 31.713032 31.264965 31.563118 31.39737  31.845879 31.453028\n",
      " 31.59089  31.920313 31.362192 31.467579 31.299763 31.300926 31.755959\n",
      " 31.534388 31.323652 31.892288 31.444437 31.813814 31.490906 31.754086\n",
      " 31.421745 31.65338  31.686796 31.316004 31.167969 31.59358  31.37983\n",
      " 31.511984 31.631577 31.324966 31.25462  31.765131 31.148376 31.708366\n",
      " 30.956268 31.60471  31.472767 31.76582  31.369825 31.76879  31.17522\n",
      " 31.49197  31.6322   31.870075 31.329367 31.50401  31.931805 31.806923\n",
      " 30.891188 31.312187 31.474604 31.478079 31.689915 31.388046 31.162722\n",
      " 31.62297  31.683718 31.85988  31.441357 31.728405 31.796629 31.809395\n",
      " 31.460163 31.406757 31.434927 31.92444  31.86266  31.937551 31.473032\n",
      " 31.394592 31.37223  31.857668 31.6173   31.072226 31.61553  31.024\n",
      " 30.928257 31.346678 30.898235 31.661434 31.418148 31.630814 31.368923\n",
      " 31.717571 31.301126 31.785252 31.60667  31.295506 31.56153  31.193392\n",
      " 31.377253 31.575542 31.904917 31.85167  31.545195 31.667978 31.87933\n",
      " 31.341724 31.352566 31.698042 31.430992 31.52262  31.480179 31.847462\n",
      " 31.952723 31.734007 31.528053 31.247366 31.12301  31.571316 31.464977\n",
      " 31.303665 31.801796 31.384129 31.384819 32.05073  31.48055  31.832096\n",
      " 31.620043 31.732048 31.490543 31.873962 31.345251 31.715052 31.898544\n",
      " 31.776861 31.55707  31.737808 31.15118  31.222029 31.69525  31.442009\n",
      " 31.573603 31.825882 31.603165 31.549957 31.525095 31.58188  31.61607\n",
      " 31.254757 31.880817 31.786043 31.607628 31.61123  31.084314 31.693428\n",
      " 31.659698 31.583517 31.597464 31.895916 31.71604  31.543703 31.804422\n",
      " 31.07333  31.71111  31.082253 31.832512 31.920588 31.594963 31.867308\n",
      " 31.69647  31.43058  31.781975 31.642551 31.258457 31.488493 31.234695\n",
      " 31.810589 31.262148 31.186625 31.5739   31.40607  31.88809  31.181944\n",
      " 31.500767 31.264614 31.585203 30.703014 31.556522 31.615543 31.832693\n",
      " 31.416536 31.8583   31.566818 31.791786 31.621422 31.777025 31.976894\n",
      " 31.325321 31.688326 31.634945 31.150688 31.63591  31.399132 31.554695\n",
      " 31.753025 31.72946  31.366215 31.597116 31.314167 31.615381 31.788748\n",
      " 31.966509 31.362188 31.533005 31.567898 31.661184 31.941574 31.56547\n",
      " 31.675678 31.612064 31.594868 31.360186 31.402065 31.349262 31.363808\n",
      " 31.677254 31.572033 31.566345 31.596998 31.478745 31.638245 31.989319\n",
      " 31.66458  31.833395 31.817274 31.19083  31.485607 32.03704  31.465134\n",
      " 31.60522  31.61155  31.663559 31.640314 31.642008 31.929869 31.921612\n",
      " 31.78864  31.532751 31.722677 31.59055  31.849869 31.85551  31.384537\n",
      " 31.622915 31.33126  31.569363 31.33554  31.57064  31.727478 31.69661\n",
      " 31.478725 31.39339  31.99719  31.66568  31.447948 31.560314 31.44344\n",
      " 31.875074 31.714987 31.763111 31.329777 31.079071 31.415112 31.309729\n",
      " 31.108727 31.626505 31.778265 31.65963  31.18243  31.615921 31.117767\n",
      " 31.447552 31.627697 31.396671 31.152948 31.817457 31.728628 31.409817\n",
      " 31.697155 31.536083 31.32426  31.316244 31.980526 31.455275 31.73693\n",
      " 31.922834 31.857384 31.525682 31.62196  31.605604 31.29018  31.542849\n",
      " 31.797298 31.27266  31.657122 31.77915  32.017353 31.399853 31.786074\n",
      " 31.642324 31.420925 31.542522 31.509075 31.627012 31.281229 31.541773\n",
      " 31.44285  31.61113  31.748827 31.631416 31.458958 31.745823 31.724098\n",
      " 31.37729  31.47089  31.54341  31.429867 31.165379 31.575745 31.65245\n",
      " 31.219624 31.128658 31.84549  31.4185   31.7982   31.106838 31.83293\n",
      " 31.54989  31.691477 31.126554 31.731981 31.418125 31.53482  31.903402\n",
      " 31.813599 31.353228 31.74457  31.295122 31.425873 31.711407 31.293114\n",
      " 31.422668 31.539425 31.143589 31.873535 31.409334 31.95629  31.457521\n",
      " 31.657364 31.389404 31.436213 31.64933  31.709568 31.726263 31.363756\n",
      " 31.394932 31.774086 31.357182 31.797234 31.865742 31.166008 31.232536\n",
      " 31.656593 31.274565 31.127811 31.338945 31.692612 31.3639   31.693695\n",
      " 31.17664  31.339731 31.577118 31.528141 31.820898 31.599226 31.258202\n",
      " 31.605635 31.008427 31.512894 31.097427 31.52058  31.062202 31.402203\n",
      " 31.142902 31.710022 31.054356 31.876343 31.189682 31.512096 31.454855\n",
      " 31.56916  30.953674 31.686298 31.099838 31.637516 31.925156 31.579369\n",
      " 31.559727 31.629568 31.220158 31.381317 31.523369 31.828175 31.476597\n",
      " 30.88947  31.498432 31.447817 31.220272 31.79859  31.54799  31.781923\n",
      " 31.10992  31.2401   31.814096 31.392332 31.105099 31.485287 31.597963\n",
      " 31.641596 31.239204 31.2425   31.61215  31.514967 31.602453 31.287235\n",
      " 31.51836  31.846123 31.780952 31.836605 30.929705 31.814594 31.726952\n",
      " 31.746918 31.393314 31.939003 31.143415 31.360256 31.211376 31.433044\n",
      " 31.642395 31.498844 31.358532 31.48655  31.5088   31.741598 31.349564\n",
      " 31.448444 31.52702  31.308512 31.362995 31.368809 31.688337 31.595568\n",
      " 31.293379 31.005592 31.523045 31.561832 31.904512 31.417984 31.46732\n",
      " 31.504417 31.618195 31.279018 31.462938 31.372684 31.517897 31.49291\n",
      " 31.585075 31.49243  31.729729 31.799496 31.58077  31.7562   31.226322\n",
      " 31.219666 31.55868  31.47154  31.292618 31.604702 31.387886 31.697357\n",
      " 31.81028  31.463223 31.796692 31.682316 31.495037 31.750475 31.582024\n",
      " 31.607716 31.257233 31.317944 31.371725 31.502686 31.256962 31.389915\n",
      " 31.31704  31.56685  31.170033 31.447485 31.114563 31.498676 31.577766\n",
      " 31.5517   31.29048  31.571661 31.378178 31.445358 31.181593 31.241282\n",
      " 31.194176 31.631004 31.783752 31.926027 31.516304 31.454895 31.181793\n",
      " 31.791903 31.344296 31.676584 31.898687 31.225052 31.409756 31.280355\n",
      " 31.330893 31.665764 31.296703 31.607351 31.68287  31.532143 31.467995\n",
      " 31.676968 31.373997 31.858898 31.469814 31.464312 31.281052 31.538109\n",
      " 31.550568 31.410345 31.898687 31.73762  31.460297 31.69576  31.012169\n",
      " 31.276598 31.51614  31.552704 31.68326  31.766813 31.327068 31.554459\n",
      " 31.389072 31.401106 31.788511 31.467224 31.372385 31.55983  31.198997\n",
      " 31.552605 31.448874 31.58038  31.62175  31.330462 31.714954 31.570892\n",
      " 31.557575 31.617125 31.854845 31.642197 31.73486  31.858328 31.303015\n",
      " 31.47372  31.312286 31.038336 31.335182 31.646963 31.571339 31.393091\n",
      " 31.496243 31.45094  31.710215 31.508764 31.83254  31.682522 31.742107\n",
      " 31.519455 31.723866 31.153519 31.954222 31.535122 31.389265 30.890802\n",
      " 31.667711 31.479294 31.806492 31.641947 31.474121 31.512491 31.89489\n",
      " 31.568218 31.394413 31.561792 31.631044 31.48325  31.451338 31.397783\n",
      " 31.594604 31.276102 31.97891  31.618862 31.58541  31.704842 31.438347\n",
      " 31.828396 31.580647 31.752905 31.432318 31.519463 31.883179 31.092201\n",
      " 31.476728 31.199028 31.424026 31.565569 31.64847  31.912119 31.326845\n",
      " 31.568174 31.341614 31.739323 31.281319 31.50557  31.698631 31.149767\n",
      " 31.338066 31.191412 31.441523 31.295298 31.27335  31.411373 31.997677\n",
      " 31.65807  31.29776  31.689072 31.431374 31.712421 31.666622], shape=(1000,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "if AUTOENCODER_TYPE == 'variational':\n",
    "    print(tf.reduce_sum(ae.layers[0].latent_entropy(x_train[:1000]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14063/14063 [==============================] - 110s 8ms/step - loss: 0.0630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1f754af150>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train[:NUM_DATA], x_train[:NUM_DATA]))\n",
    "ds = ds.shuffle(10000).repeat(30).batch(128)\n",
    "ae.fit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9389819010416667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ae, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[24.54154  23.39064  25.28295  24.913343 25.877317 24.591272 23.669014\n",
      " 23.78318  26.664965 25.233448 24.757057 24.704992 19.393684 24.598286\n",
      " 25.445442 25.291134 27.00357  24.915255 27.727304 26.123865 20.472404\n",
      " 22.51993  27.325256 24.880932 25.60801  21.189137 27.221899 21.267912\n",
      " 20.657253 25.92553  24.182734 22.169323 25.174713 24.33881  22.95702\n",
      " 25.066357 23.708462 22.534674 25.138596 25.747425 25.616364 26.411896\n",
      " 27.111038 25.854595 27.361473 25.505678 26.92191  24.515018 27.00504\n",
      " 24.162426 26.66613  19.595694 22.922808 26.190466 23.222813 23.90604\n",
      " 19.821182 25.496983 22.132847 24.270634 20.763687 26.215668 22.522182\n",
      " 21.265999 24.683502 25.212963 24.797626 26.766823 23.122677 22.290133\n",
      " 22.924213 25.351528 25.9763   25.683178 25.193972 23.184036 25.594353\n",
      " 25.343586 25.314095 25.40687  24.37595  22.750732 21.914755 25.399258\n",
      " 25.50604  25.287031 23.468128 26.348156 21.532822 22.89574  22.40924\n",
      " 23.713652 26.839146 26.753492 24.99895  22.18211  25.369232 24.244143\n",
      " 26.302319 27.017694 25.6114   23.895042 24.67823  25.359287 25.764994\n",
      " 25.002682 25.61338  22.220837 23.029156 25.789454 23.624485 20.229168\n",
      " 27.225018 25.45691  20.408192 26.09719  24.955208 24.02477  24.403782\n",
      " 20.604618 21.08154  19.457457 24.089344 23.721657 27.630302 25.85707\n",
      " 24.813843 24.775776 25.37556  24.394024 26.01137  24.938274 25.941648\n",
      " 25.834085 25.078491 21.801598 25.507618 24.966015 24.235336 24.974552\n",
      " 27.073275 25.068474 27.771355 24.261257 27.414612 25.812183 24.209797\n",
      " 24.006126 27.531769 23.558382 24.821688 23.538818 25.237041 26.097576\n",
      " 25.580479 24.528498 25.363857 22.987074 26.04805  23.754988 25.892445\n",
      " 22.720726 25.543304 26.004536 19.721252 22.962322 26.077923 25.468483\n",
      " 26.003687 22.447453 24.68058  24.41726  23.75011  25.603296 26.558903\n",
      " 24.064747 25.408186 25.896132 24.495338 20.732939 26.289597 23.264956\n",
      " 24.447083 25.876507 25.60146  22.919035 26.201221 21.946316 23.982655\n",
      " 23.044727 23.02801  27.545145 23.098232 24.163456 23.268532 25.166113\n",
      " 23.07638  25.958572 23.503307 25.873644 24.239792 24.806334 26.408062\n",
      " 23.60135  24.069378 25.225018 21.650013 25.649347 25.388264 20.595346\n",
      " 26.181587 25.340786 25.997559 21.075207 21.517744 23.489525 25.341751\n",
      " 26.196058 24.490341 23.205492 20.377192 21.35704  20.018127 24.508158\n",
      " 25.639118 24.637718 25.586712 25.732737 25.010706 19.539076 26.49478\n",
      " 24.885353 27.760485 22.378033 22.57507  22.574947 24.283699 24.290306\n",
      " 25.36293  24.10117  27.277487 21.951866 22.136906 24.003351 22.702007\n",
      " 23.855469 21.478704 26.13377  23.097961 21.8859   25.606968 25.94647\n",
      " 24.002274 18.591211 25.088358 22.476576 26.263763 25.506004 26.61433\n",
      " 23.91847  26.489582 23.783108 26.65604  23.75018  25.839249 23.50137\n",
      " 23.749363 26.085922 24.807003 25.503479 28.339811 26.230904 22.930485\n",
      " 24.253902 24.962921 24.950356 26.652279 21.278994 24.234806 23.775421\n",
      " 27.511377 21.392056 24.975466 20.79423  21.423035 25.418049 22.598396\n",
      " 23.700743 25.603458 25.916447 25.995874 23.967516 26.845871 20.990528\n",
      " 17.797836 25.191944 24.075668 24.85908  20.963547 22.657192 26.398968\n",
      " 25.19981  24.17318  21.430649 26.613176 25.39224  24.815338 25.078247\n",
      " 21.303291 26.800274 27.021992 23.13804  24.096268 26.21424  26.283712\n",
      " 25.064014 24.600193 21.297472 25.59033  25.676497 21.711025 24.088188\n",
      " 25.397896 21.391773 27.223066 20.333485 22.95634  21.879795 23.727112\n",
      " 24.1623   24.44532  23.353527 24.086346 23.957878 24.770805 26.150043\n",
      " 26.437468 24.01344  26.847063 20.673082 25.685604 21.166138 26.936481\n",
      " 25.826612 26.045399 25.309973 26.163395 19.907097 24.151203 23.826946\n",
      " 25.696856 24.473934 22.04523  25.444584 25.538284 25.187798 26.157223\n",
      " 25.188501 26.351301 23.730785 20.81501  24.399456 25.035427 25.960068\n",
      " 27.23091  21.82141  26.830442 21.910372 27.078297 26.791496 25.09001\n",
      " 22.209682 27.413551 26.146708 24.666817 19.991837 26.159563 23.761597\n",
      " 21.551659 26.48629  24.072607 22.843672 24.257706 24.710768 26.901814\n",
      " 17.651394 22.676323 24.982422 26.891228 25.177227 25.169357 20.313759\n",
      " 24.182228 25.673649 26.445278 23.625553 27.83408  25.248722 26.935547\n",
      " 21.409986 22.14907  25.742977 26.454601 22.706303 24.358885 22.760895\n",
      " 23.763489 23.856194 26.89721  26.549843 25.303616 24.977697 28.862907\n",
      " 25.19299  25.425846 22.808453 26.272717 26.073137 29.495892 25.152697\n",
      " 25.928581 23.542747 26.267506 25.40915  22.293772 25.64049  20.133862\n",
      " 20.766499 24.5004   19.36565  27.642288 23.726078 24.105251 23.105732\n",
      " 26.889114 22.734322 27.9347   25.195866 24.504314 24.005322 19.18343\n",
      " 25.744371 27.487478 25.576729 26.104145 23.097466 26.218895 25.223642\n",
      " 22.005548 24.19772  25.840614 24.678516 22.637146 23.91807  27.513653\n",
      " 26.357536 28.1866   22.130323 20.392868 25.933937 25.225012 23.604622\n",
      " 19.653812 25.92185  19.66105  22.700089 27.116879 21.728302 26.680727\n",
      " 24.642273 26.682209 24.499622 27.106916 20.494728 26.691692 25.01528\n",
      " 27.64875  23.279003 22.637875 25.689753 24.173933 25.806501 26.963467\n",
      " 21.604408 27.117764 25.155058 22.755285 25.86974  24.158703 24.961063\n",
      " 23.586845 24.821547 25.143497 23.867054 22.922443 24.036808 25.804956\n",
      " 24.283419 25.845055 25.824928 25.99364  26.358055 27.068134 24.574411\n",
      " 20.802572 24.474697 22.395245 25.011152 25.071785 24.613188 26.181175\n",
      " 23.69122  25.259188 23.99036  26.442097 20.31096  24.050488 24.123848\n",
      " 26.620401 20.664532 24.71949  22.281475 22.780327 25.982635 21.396275\n",
      " 23.664185 19.405052 24.125229 21.926914 25.018742 24.431961 26.418503\n",
      " 24.254444 24.622807 23.373613 23.166164 26.583546 23.86183  26.44101\n",
      " 22.663319 26.988012 24.40397  23.200117 25.759172 23.482138 24.634798\n",
      " 24.00948  25.024876 21.951773 23.144245 26.25485  22.442299 25.245937\n",
      " 26.062082 23.04306  23.419405 25.062672 22.250134 27.368073 23.475988\n",
      " 26.101875 23.590624 23.73068  22.993042 25.153696 23.940014 25.626884\n",
      " 24.756409 24.655895 23.891426 21.291245 24.017906 25.492285 25.731754\n",
      " 27.224873 25.306313 25.41637  20.737114 25.49376  25.249363 25.25819\n",
      " 24.468727 24.205612 26.157427 28.533777 25.8185   27.289927 25.325325\n",
      " 26.72628  24.29019  27.876286 22.816116 25.462902 25.111124 24.514206\n",
      " 25.376259 22.833101 23.609802 22.922638 24.71827  26.44699  25.760853\n",
      " 25.675858 20.535563 27.87435  26.505482 20.009972 26.062866 23.00476\n",
      " 25.128036 23.109615 25.011032 23.068207 26.168446 23.621416 24.991505\n",
      " 23.34606  23.963665 25.313915 23.741264 22.097446 25.458748 22.595333\n",
      " 23.21438  25.298073 24.649923 21.989418 26.235933 27.75642  24.195225\n",
      " 26.192604 25.106962 19.583061 21.552547 27.338242 23.93535  26.88601\n",
      " 24.992565 26.76912  22.501743 21.571342 25.361217 21.77156  24.104046\n",
      " 26.325901 26.55152  25.769062 27.573544 26.312244 23.105095 23.555983\n",
      " 22.546568 24.64345  25.820568 25.122093 23.55713  23.029377 23.574505\n",
      " 22.408182 25.289469 26.763062 26.412933 24.59311  24.17433  24.286694\n",
      " 22.563158 23.534801 22.340786 22.5211   20.272346 22.321255 25.25691\n",
      " 23.650387 20.190899 27.433378 23.090572 25.995192 21.407867 26.353111\n",
      " 24.762085 27.65016  22.143196 26.639091 22.824472 22.700935 25.542278\n",
      " 26.400982 25.831755 24.714252 20.579563 25.219555 25.212528 24.538614\n",
      " 23.153854 26.20966  23.128866 25.778994 21.408598 26.257542 22.604877\n",
      " 26.771755 20.546162 21.952518 23.661768 23.162256 24.266003 24.642511\n",
      " 25.75108  26.603626 20.577047 25.256973 24.765827 21.316917 19.690239\n",
      " 26.122343 23.2299   24.399988 23.358408 25.534912 22.510172 25.818468\n",
      " 21.07825  22.208376 23.017426 21.963915 24.056393 25.764427 25.586811\n",
      " 27.565475 21.513298 26.126057 22.77356  21.481304 21.826715 22.723381\n",
      " 21.523914 28.456411 22.704662 26.664843 21.834532 25.46722  25.826447\n",
      " 25.88105  17.807365 26.882812 17.766602 26.582436 24.49747  25.78926\n",
      " 23.38161  25.553398 24.044151 23.30237  23.276157 28.50251  24.860535\n",
      " 22.13403  23.535698 25.450207 22.72499  25.659416 24.65942  26.075106\n",
      " 22.96242  22.138885 25.000696 23.467278 22.85144  25.74995  23.23821\n",
      " 27.65011  23.012413 23.39209  24.658363 21.282562 26.11197  23.391508\n",
      " 24.169964 28.986582 23.881796 24.711117 18.872355 28.16215  25.264938\n",
      " 27.245314 22.472134 25.572483 21.302563 25.705734 23.626656 21.95319\n",
      " 24.206633 24.361778 24.961323 21.988731 24.078342 28.618626 24.10803\n",
      " 25.02454  21.45914  25.06928  24.095135 24.5988   25.793713 24.451435\n",
      " 23.836967 25.254856 25.519554 25.980614 25.753857 22.164886 23.227013\n",
      " 25.817392 25.02132  20.009293 23.214561 23.691757 24.47919  24.430756\n",
      " 23.67123  22.371881 23.243027 26.45351  24.71815  26.050312 20.43976\n",
      " 21.345005 26.101107 20.75349  23.514805 23.743933 22.967815 25.584318\n",
      " 24.849775 26.324402 25.498877 25.854992 24.878313 25.749317 24.898998\n",
      " 23.462599 21.57127  22.369888 23.455538 23.558147 21.890812 23.665525\n",
      " 23.750229 24.341942 19.756195 24.536287 22.532986 23.477615 24.900307\n",
      " 24.891195 22.294325 25.976171 21.967138 26.58359  21.141258 23.66345\n",
      " 23.262308 23.299177 26.163303 27.18623  24.811623 23.015297 22.315546\n",
      " 26.655409 22.860626 24.603401 25.569859 18.906227 21.382084 22.119064\n",
      " 21.09505  25.49215  23.915436 27.50398  26.514801 24.748798 23.866507\n",
      " 22.559507 19.69154  26.388687 24.26271  22.054508 22.767998 25.549944\n",
      " 21.403584 23.76642  25.569859 27.57932  20.514097 25.099419 20.978764\n",
      " 21.440205 23.91209  27.663403 27.251585 28.405983 23.236809 24.170738\n",
      " 23.08725  22.968151 23.688269 24.39119  21.369661 21.083685 19.87468\n",
      " 24.53004  22.354992 25.832447 23.212393 24.669565 26.13692  23.54327\n",
      " 21.512661 24.886742 24.895939 25.072102 25.27475  27.807217 21.883165\n",
      " 21.084778 22.921995 22.21097  21.40467  25.15591  25.270489 24.156809\n",
      " 21.99538  24.755589 25.2431   22.516962 24.241581 25.8022   25.752998\n",
      " 25.512955 26.135368 23.909262 26.743507 21.851728 25.089783 21.087215\n",
      " 25.760323 23.087631 25.699823 24.503492 22.080673 25.800571 24.258242\n",
      " 24.22572  26.36306  25.14751  25.10205  23.340328 25.121216 23.940224\n",
      " 23.313398 22.628662 26.50021  23.870401 23.89149  23.794744 23.284409\n",
      " 25.530064 24.807709 26.700432 22.138453 23.83051  26.008509 23.725391\n",
      " 24.482872 25.226978 20.201561 25.389269 24.435905 28.16614  22.399162\n",
      " 26.464254 20.971113 25.379375 25.44289  22.816511 25.664019 21.042439\n",
      " 24.811607 24.03926  21.61051  22.134357 24.993137 23.414501 27.451637\n",
      " 21.263527 22.320044 25.01469  21.652082 25.714333 23.651768], shape=(1000,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "if AUTOENCODER_TYPE == 'variational':\n",
    "    print(tf.reduce_sum(ae.layers[0].latent_entropy(x_train[:1000]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
