{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# For integer quantization:\n",
    "# !pip install tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from mnist import load_mnist, evaluate\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global configurations\n",
    "\n",
    "IMAGE_SIZE = (16, 16)\n",
    "BINARIZE = True\n",
    "AUTOENCODER_TYPE = ('vanilla', 'variational')[0]\n",
    "NUM_DATA = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), _ = load_mnist(image_size=IMAGE_SIZE, binarize=BINARIZE)\n",
    "x_train = (x_train + 1) / 2  # x \\in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(x, threshold):\n",
    "    \"\"\"Returns 1 if x > threshold else 0, element-wisely.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tensor\n",
    "    threshold : float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The same shape and dtype as x.\n",
    "    \"\"\"\n",
    "    y = tf.where(x > threshold, 1, 0)\n",
    "    y = tf.cast(y, x.dtype)\n",
    "    return y\n",
    "\n",
    "\n",
    "def softly_binarize(x, threshold, from_logits=False):\n",
    "    \"\"\"Returns 1 if x > threshold else 0, element-wisely, with the gradients\n",
    "    :math:`\\partial f_i / \\partial x_j = \\delta_{i j}`, i.e. an unit Jacobian.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tensor\n",
    "    threshold : float\n",
    "    from_logits : bool, optional\n",
    "        If true, then softly binarize sigmoid(x) instead of x.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        The same shape and dtype as x.\n",
    "    \"\"\"\n",
    "\n",
    "    def identity(dy):\n",
    "        return dy\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def fn(x):\n",
    "        y = binarize(x, threshold)\n",
    "        return y, identity\n",
    "\n",
    "    return fn(tf.nn.sigmoid(x)) if from_logits else fn(x)\n",
    "\n",
    "\n",
    "class SoftBinarization(tf.keras.layers.Layer):\n",
    "    \"\"\"For using in tf.keras.Sequential.\n",
    "\n",
    "    If in training phase, then do nothing. Otherwise, make soft binarization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float\n",
    "    from_logits : bool, optional\n",
    "        If true, then softly binarize sigmoid(x) instead of x.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold, from_logits=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['threshold'] = self.threshold\n",
    "        config['from_logits'] = self.from_logits\n",
    "        return config\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if self.from_logits:\n",
    "            x = tf.nn.sigmoid(x)\n",
    "        if training:\n",
    "            return x\n",
    "        return softly_binarize(x, self.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "    \"\"\"Multi-layer perceptron (MLP).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : [int]\n",
    "    activation : callable or string, optional\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['activation'] = self.activation\n",
    "        return config\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        layers = [tf.keras.layers.Dense(n, 'relu') for n in self.units[:-1]]\n",
    "        layers.append(tf.keras.layers.Dense(self.units[-1], self.activation))\n",
    "        self._ffn = tf.keras.Sequential(layers)\n",
    "        self._ffn.build(batch_input_shape)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self._ffn(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentBernoulliVanillaAutoencoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : [int]\n",
    "        Hidden units along the encoder. The decoder use symmetric structure.\n",
    "    activation : callable or string, optional\n",
    "        Final output activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "        self._encoder = MLP(units, 'sigmoid')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['activation'] = self.activation\n",
    "        return config\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        ambient_dim = batch_input_shape[-1]\n",
    "        units = self.units[::-1][1:] + [ambient_dim]  # symmetric structure\n",
    "        self._decoder = MLP(units, self.activation)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self._encoder(x)\n",
    "        z = softly_binarize(z, threshold=0.5)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self._decoder(z)\n",
    "        return x\n",
    "\n",
    "    def call(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentBernoulliVariationalAutoencoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    References\n",
    "    ----------\n",
    "    1. https://davidstutz.de/bernoulli-variational-auto-encoder-in-torch/\n",
    "    2. Information Theory, Inference, and Learning Algorithms, D. Mackay,\n",
    "       section 33.2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : [int]\n",
    "        Hidden units along the encoder. The decoder use symmetric structure.\n",
    "    activation : callable or string, optional\n",
    "        Final output activation.\n",
    "    temperature : float, optional\n",
    "    num_samples : int, optional\n",
    "        Number of samples for Monte-Carlo integral.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, temperature=0,\n",
    "                 num_samples=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.temperature = float(temperature)\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        self._encoder = MLP(units, name='encoder')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['activation'] = self.activation\n",
    "        config['temperature'] = self.temperature\n",
    "        config['num_samples'] = self.asciinum_samples\n",
    "        return config\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        ambient_dim = batch_input_shape[-1]\n",
    "        units = self.units[::-1][1:] + [ambient_dim]  # symmetric structure\n",
    "        self._decoder = MLP(units, self.activation, name='decoder')\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def encode(self, x, training=None):\n",
    "        \"\"\"Add regularizer if training.\"\"\"\n",
    "        latent_logits = self._encoder(x)\n",
    "\n",
    "        if training:\n",
    "            sampled_latent_logits = self._reparam_trick(\n",
    "                latent_logits, self.num_samples)\n",
    "            latent_entropy = self._latent_entropy(sampled_latent_logits)\n",
    "            self.add_loss(- self.temperature * tf.reduce_mean(latent_entropy))\n",
    "\n",
    "        else:\n",
    "            sampled_latent_logits = self._reparam_trick(latent_logits, 1)\n",
    "\n",
    "        # use the first sample as the result to return\n",
    "        z = softly_binarize(tf.nn.sigmoid(sampled_latent_logits[0]),\n",
    "                            threshold=0.5)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self._decoder(z)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        z = self.encode(x, training)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon\n",
    "\n",
    "    @staticmethod\n",
    "    def _reparam_trick(logits, num_samples):\n",
    "        \"\"\"\n",
    "        Notes\n",
    "        -----\n",
    "        s: random seed\n",
    "        p: Bernoulli probability\n",
    "        g(s, p): re-parameterization trick for Bernoulli distribution\n",
    "            E_{z ~ bernoulli(p=f(x; w))} [...(z)]\n",
    "            -> E_{s ~ uniform(0, 1)} [...(z=g(s, p=f(x;w)))],\n",
    "            where gradient(g(s, p), p) exists.\n",
    "\n",
    "        Lemma:\n",
    "            s ~ uniform(0, 1)\n",
    "            a = s / (1 - s) * p / (1 - p)\n",
    "            z = 1 if a > 1 else 0\n",
    "            => z ~ bernoulli(p)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : tensor\n",
    "            Shape [batch_size, depth...].\n",
    "        num_samples : int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "            Shape [num_samples, batch_size, depth...].\n",
    "        \"\"\"\n",
    "        # seed\n",
    "        eps = 1e-8\n",
    "        s = tf.random.uniform(shape=([num_samples, 1] + logits.shape[1:]),\n",
    "                              minval=eps, maxval=1-eps)  # [S, B, D...]\n",
    "\n",
    "        logits = logits[tf.newaxis, ...]  # [1, B, D...]\n",
    "        # employ the relation log(sigmoid(x)) - log(1 - sigmoid(x)) = x\n",
    "        sampled_logits = tf.math.log(s) - tf.math.log(1 - s) + logits\n",
    "        return sampled_logits  # [S, B, D...]\n",
    "\n",
    "    @staticmethod\n",
    "    def _latent_entropy(latent_logits):\n",
    "        \"\"\"Entropy of the latent variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        latent_logits : tensor\n",
    "            Shape [num_samples, batch_size, depth...].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tensor\n",
    "            Shape [batch_size, depth...].\n",
    "        \"\"\"\n",
    "        p = tf.nn.sigmoid(latent_logits)\n",
    "        log_p = log_sigmoid(latent_logits)\n",
    "        log_1mp = log_1m_sigmoid(latent_logits)\n",
    "        entropy = tf.reduce_mean(\n",
    "            -p * log_p - (1 - p) * log_1mp,\n",
    "            axis=0)\n",
    "        return entropy\n",
    "\n",
    "    def latent_entropy(self, x):\n",
    "        latent_logits = self._encoder(x)\n",
    "        sampled_latent_logits = self._reparam_trick(\n",
    "            latent_logits, self.num_samples)\n",
    "        return self._latent_entropy(sampled_latent_logits)\n",
    "\n",
    "\n",
    "def log_sigmoid(x):\n",
    "    \"\"\"log(sigmoid(x)) = x - softplus(x)\"\"\"\n",
    "    return x - tf.nn.softplus(x)\n",
    "\n",
    "\n",
    "def log_1m_sigmoid(x):\n",
    "    \"\"\"log(1 - sigmoid(x)) = - softplus(x)\"\"\"\n",
    "    return - tf.nn.softplus(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [tf.keras.Input([IMAGE_SIZE[0] * IMAGE_SIZE[1]])]\n",
    "if AUTOENCODER_TYPE == 'vanilla':\n",
    "    layers += [\n",
    "        LatentBernoulliVanillaAutoencoder([64], 'sigmoid'),\n",
    "    ]\n",
    "elif AUTOENCODER_TYPE == 'variational':\n",
    "    layers += [\n",
    "        LatentBernoulliVariationalAutoencoder([64], 'sigmoid',\n",
    "                                              temperature=1e-1),\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError()\n",
    "if BINARIZE:\n",
    "    layers.append(SoftBinarization(0.5))\n",
    "ae = tf.keras.Sequential(layers)\n",
    "ae.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUTOENCODER_TYPE == 'variational':\n",
    "    print(ae.layers[0].latent_entropy(x_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14063/14063 [==============================] - 59s 4ms/step - loss: 0.1013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f764c2d3410>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train[:NUM_DATA], x_train[:NUM_DATA]))\n",
    "ds = ds.shuffle(10000).repeat(30).batch(128)\n",
    "ae.fit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9706830729166667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ae, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUTOENCODER_TYPE == 'variational':\n",
    "    print(ae.layers[0].latent_entropy(x_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../dat/’: File exists\n",
      "total 15M\n",
      "-rwxrwxrwx 1 pxj pxj 15M Sep 22 10:35 training_z.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "training_z = ae.layers[0].encode(x_train)\n",
    "\n",
    "!mkdir ../dat/\n",
    "with open('../dat/training_z.pkl', 'wb') as f:\n",
    "    pickle.dump(training_z, f)\n",
    "!ls -lhtr ../dat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization fails:\n",
    "\n",
    "# import tensorflow_model_optimization as tfmot\n",
    "# import re\n",
    "\n",
    "# def get_class_name(layer):\n",
    "#     s = str(type(layer))\n",
    "#     m = re.search(r\"<class '.*\\.([a-zA-Z]+)'>\", s)\n",
    "#     return m.group(1)\n",
    "\n",
    "\n",
    "# def clone_layer(layer):\n",
    "#     return type(layer)(**layer.get_config())\n",
    "\n",
    "\n",
    "# LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "# MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "# class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "#     # Configure how to quantize weights.\n",
    "#     def get_weights_and_quantizers(self, layer):\n",
    "#         return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "#     # Configure how to quantize activations.\n",
    "#     def get_activations_and_quantizers(self, layer):\n",
    "#         return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "#     def set_quantize_weights(self, layer, quantize_weights):\n",
    "#         # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "#         # , in the same order\n",
    "#         layer.kernel = quantize_weights[0]\n",
    "\n",
    "#     def set_quantize_activations(self, layer, quantize_activations):\n",
    "#         # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "#         # , in the same order.\n",
    "#         layer.activation = quantize_activations[0]\n",
    "\n",
    "#     # Configure how to quantize outputs (may be equivalent to activations).\n",
    "#     def get_output_quantizers(self, layer):\n",
    "#         return []\n",
    "\n",
    "#     def get_config(self):\n",
    "#         return {}\n",
    "\n",
    "\n",
    "# quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "# quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "# quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "# quant_layers = [tf.keras.Input([IMAGE_SIZE[0] * IMAGE_SIZE[1]])]\n",
    "# quant_scope = {\n",
    "#     'DefaultDenseQuantizeConfig': DefaultDenseQuantizeConfig,\n",
    "# }\n",
    "# for layer in ae.layers:\n",
    "#     quant_scope[get_class_name(layer)] = type(layer)\n",
    "#     quant_layers.append(\n",
    "#         quantize_annotate_layer(\n",
    "#             clone_layer(layer),\n",
    "#             DefaultDenseQuantizeConfig()))\n",
    "\n",
    "# model = quantize_annotate_model(tf.keras.Sequential(quant_layers))\n",
    "\n",
    "# # `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
    "# # as well as the custom Keras layer.\n",
    "# with quantize_scope(quant_scope):\n",
    "#     # Use `quantize_apply` to actually make the model quantization aware.\n",
    "#     quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "# quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
